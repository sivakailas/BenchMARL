# -*- coding: utf-8 -*-
"""Simulation_and_training_in_VMAS_and_BenchMARL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/proroklab/VectorizedMultiAgentSimulator/blob/main/notebooks/Simulation_and_training_in_VMAS_and_BenchMARL.ipynb

This notebook will show you how to create you custom VMAS scenario from scratch and train it in BenchMARL.

We will create a scenario where multiple robots with different embodiments need to navigate to their goals while avoiding each other and obstacles.

The result will look something like this:

![](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/tutorial_gifs/Trainwithphysicallyheterogeneousagents.gif)

You can run both sections together or each one independently.

# VMAS: creating a custom scenario

VMAS is a vectorized multi-agent simulator but also a collection of scenarios.

The best way to learn how VMAS works is to implement your own scenario, which is the focus of this section of the notebook.

![vmas_diagram](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/VMAS_diagram.png)

## Installing depdendencies
"""

# # @title Rendering dependencies
# !apt-get update
# !apt-get install -y x11-utils python3-opengl xvfb
# !pip install pyvirtualdisplay
# import pyvirtualdisplay
# display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))
# display.start()

# # @title Install vmas
# !pip install vmas

"""## Creating a scenario

To implement a scenario, there are some compulsory and optional functions that need to be created, let's see wat they are:
"""

from vmas.simulator.scenario import BaseScenario

class MyScenario(BaseScenario):

    def make_world(self, batch_dim, device, **kwargs):
        raise NotImplementedError()

    def reset_world_at(self, env_index):
        raise NotImplementedError()

    def observation(self, agent):
        raise NotImplementedError()

    def reward(self, agent):
        raise NotImplementedError()

"""We will now implement and explain them one by one.

The scenario we will create is a version of multi-agent [navigation](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/scenarios/navigation.gif).

In this scenario, randomly spawned agents (colored shapes with surrounding dots) need to navigate to randomly spawned goals (smaller circles of the same color as the agent). Agents need to use LIDARs (dots around them telling distances to other agents) to avoid colliding into each other. They also need to avoid collisions with surrounding obstacles (black circles). Agents act in a 2D continuous world with drag and elastic collisions.

In VMAS, the `entity` is the base concept. Entities can be agents or landmarks.
The difference between the 2 is that agents can act and receive rewards and observations.

We will discuss in depth actions, rewards, and observations in the respective sections.

This scenario creation tutorial **WILL** showcase the following VMAS features:
- The voctorized nature of VMAS
- The ability to simulate agents with different dynamics (we will use holonomic, differential drive, and car agents)
- The ability to add sensors to the agents (we will use LIDARs)
- The ability of simulating physics and collisions for agents with different shapes

This scenario creation tutorial **WILL NOT** showcase the following VMAS features:
- Joints (see the waterfall debug scenario for an example)
- Custom gravity
- The differentiability of the simulator
- Customising friction and drag
- Using controllers to change the input (e.g., using velocities instead of forces for input)
- Using heuristic (scripted) controllers for some of the agents
- Plotting a function under the task rendering
- Competitive scenarios
- Many more (sorry, VMAS has a lot to offer and we have limited time)

### `make_world`

Our first function!!

Here is where we decide what will be in our world (agents, landmarks) and some configuration parameters for it.

In order, this function does:

1.   First things first, we will define the parameters of our scenario, reading from `kwargs` and definining the default values for them.
2.   Then, we will create the world object, this is the component where the physics simulation lies, containing all our agents and landmarks.
3.   Then, we add our agents, which are of 3 types: holonomic, differential drive, and car (bicycle). For each agent we define shape, collidability, action, ranges, dynamics, etc.
4.   Each agent is equipped with a LIDAR that has a filter allowing it to sense only other agents (we will give obstacle positions directly in the observations without sensing)
5.   We add a landmark for each agent representing its goal
6.   Finally, we add obstacles that the agents need to avoid
"""

import typing
from typing import Dict, List

import torch
from torch import Tensor

from vmas.simulator.core import Agent, Box, Landmark, Sphere, World
from vmas.simulator.dynamics.diff_drive import DiffDrive
from vmas.simulator.dynamics.holonomic import Holonomic
from vmas.simulator.dynamics.kinematic_bicycle import KinematicBicycle
from vmas.simulator.scenario import BaseScenario
from vmas.simulator.sensors import Lidar
from vmas.simulator.utils import (
    ANGULAR_FRICTION,
    Color,
    DRAG,
    LINEAR_FRICTION,
    ScenarioUtils,
)

if typing.TYPE_CHECKING:
    from vmas.simulator.rendering import Geom


from vmas.simulator.scenario import BaseScenario
from typing import Union
import time
import torch
from vmas import make_env
from vmas.simulator.core import Agent

def use_vmas_env(
    render: bool,
    num_envs: int,
    n_steps: int,
    device: str,
    scenario: Union[str, BaseScenario],
    continuous_actions: bool,
    **kwargs
):
    """Example function to use a vmas environment.

    This is a simplification of the function in `vmas.examples.use_vmas_env.py`.

    Args:
        continuous_actions (bool): Whether the agents have continuous or discrete actions
        scenario (str, BaseScenario): Name of scenario or scenario class
        device (str): Torch device to use
        render (bool): Whether to render the scenario
        num_envs (int): Number of vectorized environments
        n_steps (int): Number of steps before returning done

    """

    scenario_name = scenario if isinstance(scenario,str) else scenario.__class__.__name__

    env = make_env(
        scenario=scenario,
        num_envs=num_envs,
        device=device,
        continuous_actions=continuous_actions,
        seed=0,
        # Environment specific variables
        **kwargs
    )

    frame_list = []  # For creating a gif
    init_time = time.time()
    step = 0

    for s in range(n_steps):
        step += 1
        print(f"Step {step}")

        actions = []
        for i, agent in enumerate(env.agents):
            action = env.get_random_action(agent)

            actions.append(action)

        obs, rews, dones, info = env.step(actions)

        if render:
            frame = env.render(mode="rgb_array")
            frame_list.append(frame)

    total_time = time.time() - init_time
    print(
        f"It took: {total_time}s for {n_steps} steps of {num_envs} parallel environments on device {device} "
        f"for {scenario_name} scenario."
    )

    if render:
        from moviepy.editor import ImageSequenceClip
        fps=30
        clip = ImageSequenceClip(frame_list, fps=fps)
        clip.write_gif(f'{scenario_name}.gif', fps=fps)

import torch

def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

import typing
from typing import Dict, List

import torch

from torch import Tensor

from vmas import render_interactively
from vmas.simulator.core import Agent, Box, Landmark, Sphere, World
from vmas.simulator.dynamics.diff_drive import DiffDrive
from vmas.simulator.dynamics.holonomic import Holonomic
from vmas.simulator.dynamics.kinematic_bicycle import KinematicBicycle

from vmas.simulator.scenario import BaseScenario
from vmas.simulator.sensors import Lidar

from vmas.simulator.utils import (
    ANGULAR_FRICTION,
    Color,
    DRAG,
    LINEAR_FRICTION,
    ScenarioUtils,
)

if typing.TYPE_CHECKING:
    from vmas.simulator.rendering import Geom


class MyScenario(BaseScenario):
    def make_world(self, batch_dim: int, device: torch.device, **kwargs):
        ################
        # Scenario configuration
        ################
        self.plot_grid = False  # You can use this to plot a grid under the rendering for visualization purposes

        self.n_agents_holonomic = kwargs.pop(
            "n_agents_holonomic", 2
        )  # Number of agents with holonomic dynamics
        self.n_agents_diff_drive = kwargs.pop(
            "n_agents_diff_drive", 1
        )  # Number of agents with differential drive dynamics
        self.n_agents_car = kwargs.pop(
            "n_agents_car", 1
        )  # Number of agents with car dynamics
        self.n_agents = (
            self.n_agents_holonomic + self.n_agents_diff_drive + self.n_agents_car
        )
        self.n_obstacles = kwargs.pop("n_obstacles", 0)

        self.world_spawning_x = kwargs.pop(
            "world_spawning_x", 0.25*self.n_agents
        )  # X-coordinate limit for entities spawning
        self.world_spawning_y = kwargs.pop(
            "world_spawning_y", 0.25*self.n_agents
        )  # Y-coordinate limit for entities spawning

        self.comms_rendering_range = kwargs.pop(
            "comms_rendering_range", 0
        )  # Used for rendering communication lines between agents (just visual)
        self.lidar_range = kwargs.pop("lidar_range", 0)  # Range of the LIDAR sensor
        self.n_lidar_rays = kwargs.pop(
            "n_lidar_rays", 0
        )  # Number of LIDAR rays around the agent, each ray gives an observation between 0 and lidar_range

        self.shared_rew = kwargs.pop(
            "shared_rew", False
        )  # Whether the agents get a global or local reward for going to their goals
        self.final_reward = kwargs.pop(
            "final_reward", 0.01
        )  # Final reward that all the agents get when the scenario is done
        self.agent_collision_penalty = kwargs.pop(
            "agent_collision_penalty", -1.5
        )  # Penalty reward that an agent gets for colliding with another agent or obstacle

        self.agent_radius = kwargs.pop("agent_radius", 0.1)
        self.min_distance_between_entities = (
            self.agent_radius * 2 + 0.05
        )  # Minimum distance between entities at spawning time
        self.min_collision_distance = (
            0.005  # Minimum distance between entities for collision trigger
        )

        ScenarioUtils.check_kwargs_consumed(kwargs) # Warn is not all kwargs have been consumed


        ################
        # Make world
        ################
        world = World(
            batch_dim,  # Number of environments simulated
            device,  # Device for simulation
            substeps=5,  # Number of physical substeps (more yields more accurate but more expensive physics)
            collision_force=500,  # Paramneter to tune for collisions
            dt=0.1,  # Simulation timestep
            gravity=(0.0, 0.0),  # Customizable gravity
            drag=DRAG,  # Physics parameters
            linear_friction=LINEAR_FRICTION,  # Physics parameters
            angular_friction=ANGULAR_FRICTION,  # Physics parameters
            # There are many more....
        )

        ################
        # Add agents
        ################
        known_colors = [
            Color.BLUE,
            Color.ORANGE,
            Color.GREEN,
            Color.PINK,
            Color.PURPLE,
            Color.YELLOW,
            Color.RED,
        ]  # Colors for the first 7 agents
        colors = torch.randn(
            (max(self.n_agents - len(known_colors), 0), 3), device=device
        )  # Other colors if we have more agents are random

        self.goals = []  # We will store our agent goal entities here for easy access
        for i in range(self.n_agents):
            color = (
                known_colors[i]
                if i < len(known_colors)
                else colors[i - len(known_colors)]
            )  # Get color for agent

            sensors = [
                Lidar(
                    world,
                    n_rays=self.n_lidar_rays,
                    max_range=self.lidar_range,
                    entity_filter=lambda e: isinstance(
                        e, Agent
                    ),  # This makes sure that this lidar only percieves other agents
                    angle_start=0.0,  # LIDAR angular ranges (we sense 360 degrees)
                    angle_end=2
                    * torch.pi,  # LIDAR angular ranges (we sense 360 degrees)
                )
            ]  # Agent LIDAR sensor

            if i < self.n_agents_holonomic:
                agent = Agent(
                    name=f"holonomic_{i}",
                    collide=True,
                    color=color,
                    render_action=True,
                    sensors=sensors,
                    shape=Sphere(radius=self.agent_radius),
                    u_range=[1, 1],  # Ranges for actions
                    u_multiplier=[1, 1],  # Action multipliers
                    dynamics=Holonomic(),  # If you got to its class you can see it has 2 actions: force_x, and force_y
                )
            elif i < self.n_agents_holonomic + self.n_agents_diff_drive:
                agent = Agent(
                    name=f"diff_drive_{i - self.n_agents_holonomic}",
                    collide=True,
                    color=color,
                    render_action=True,
                    sensors=sensors,
                    shape=Sphere(radius=self.agent_radius),
                    u_range=[1, 1],  # Ranges for actions
                    u_multiplier=[0.5, 1],  # Action multipliers
                    dynamics=DiffDrive(
                        world
                    ),  # If you got to its class you can see it has 2 actions: forward velocity and angular velocity
                )
            else:
                max_steering_angle = torch.pi / 4
                width = self.agent_radius
                agent = Agent(
                    name=f"car_{i-self.n_agents_holonomic-self.n_agents_diff_drive}",
                    collide=True,
                    color=color,
                    render_action=True,
                    sensors=sensors,
                    shape=Box(length=self.agent_radius * 2, width=width),
                    u_range=[1, max_steering_angle],
                    u_multiplier=[0.5, 1],
                    dynamics=KinematicBicycle(
                        world,
                        width=width,
                        l_f=self.agent_radius,  # Distance between the front axle and the center of gravity
                        l_r=self.agent_radius,  # Distance between the rear axle and the center of gravity
                        max_steering_angle=max_steering_angle,
                    ),  # If you got to its class you can see it has 2 actions: forward velocity and steering angle
                )
            agent.pos_rew = torch.zeros(
                batch_dim, device=device
            )  # Tensor that will hold the position reward fo the agent
            agent.agent_collision_rew = (
                agent.pos_rew.clone()
            )  # Tensor that will hold the collision reward fo the agent

            world.add_agent(agent)  # Add the agent to the world

            ################
            # Add goals
            ################
            goal = Landmark(
                name=f"goal_{i}",
                collide=False,
                color=color,
            )
            world.add_landmark(goal)
            agent.goal = goal
            self.goals.append(goal)

        ################
        # Add obstacles
        ################
        self.obstacles = (
            []
        )  # We will store obstacles here for easy access
        for i in range(self.n_obstacles):
            obstacle = Landmark(
                name=f"obstacle_{i}",
                collide=True,
                color=Color.BLACK,
                shape=Sphere(radius=self.agent_radius * 2 / 3),
            )
            world.add_landmark(obstacle)
            self.obstacles.append(obstacle)

        self.pos_rew = torch.zeros(
            batch_dim, device=device
        )  # Tensor that will hold the global position reward
        self.final_rew = (
            self.pos_rew.clone()
        )  # Tensor that will hold the global done reward
        self.all_goal_reached = (
            self.pos_rew.clone()
        )  # Tensor indicating if all goals have been reached

        return world

    def reset_world_at(self, env_index: int = None):
        ScenarioUtils.spawn_entities_randomly(
            self.world.agents
            + self.obstacles
            + self.goals,  # List of entities to spawn
            self.world,
            env_index,  # Pass the env_index so we only reset what needs resetting
            self.min_distance_between_entities,
            x_bounds=(-self.world_spawning_x, self.world_spawning_x),
            y_bounds=(-self.world_spawning_y, self.world_spawning_y),
        )

        for agent in self.world.agents:
            if env_index is None:
                agent.goal_dist = torch.linalg.vector_norm(
                    agent.state.pos - agent.goal.state.pos,
                    dim=-1,
                )  # Tensor holding the distance of the agent to the goal, we will use it to compute the reward
                agent.pos_shaping = (
                    torch.linalg.vector_norm(
                        agent.state.pos - agent.goal.state.pos,
                        dim=1,
                    )
                )
            else:
                agent.goal_dist[env_index] = torch.linalg.vector_norm(
                    agent.state.pos[env_index] - agent.goal.state.pos[env_index]
                )
                agent.pos_shaping[env_index] = (
                    torch.linalg.vector_norm(
                        agent.state.pos[env_index] - agent.goal.state.pos[env_index]
                    )
                )

    def reward(self, agent: Agent):
        is_first = agent == self.world.agents[0]

        if is_first:
            # We can compute rewards when the first agent is called such that we do not have to recompute global components

            self.pos_rew[:] = 0  # Reset previous reward
            self.final_rew[:] = 0  # Reset previous reward

            for a in self.world.agents:
                a.agent_collision_rew[:] = 0  # Reset previous reward
                distance_to_goal = torch.linalg.vector_norm(
                    a.state.pos - a.goal.state.pos,
                    dim=-1,
                )
                a.on_goal = distance_to_goal < a.shape.circumscribed_radius()

                # The positional reward is the delta in distance to the goal.
                # This makes it so that if the agent moves 1m towards the goal it is rewarded
                # the same amount regardless of its absolute distance to it
                # This would not be the case if pos_rew = -distance_to_goal (a common choice)
                # This choice leads to better training
                # a.pos_rew = a.goal_dist - distance_to_goal

                # a.goal_dist = distance_to_goal  # Update distance to goal
                # self.pos_rew += a.pos_rew  # Global pos reward
                self.pos_rew += self.agent_reward(a)

            # If all agents reached their goal we give them all a final_rew
            self.all_goal_reached = torch.all(
                torch.stack([a.on_goal for a in self.world.agents], dim=-1),
                dim=-1,
            )
            self.final_rew[self.all_goal_reached] = self.final_reward

            for i, a in enumerate(self.world.agents):
                # Agent-agent collision
                for j, b in enumerate(self.world.agents):
                    if i <= j:
                        continue
                    if self.world.collides(a, b):
                        distance = self.world.get_distance(a, b)
                        # if distance <= self.min_collision_distance*2:
                        #     a.pos_rew = 0
                        #     b.pos_rew = 0
                        #     self.pos_rew -= a.pos_rew
                        a.agent_collision_rew[
                            distance <= self.min_collision_distance
                        ] += self.agent_collision_penalty
                        b.agent_collision_rew[
                            distance <= self.min_collision_distance
                        ] += self.agent_collision_penalty
                # Agent obstacle collision
                for b in self.obstacles:
                    print('THERE SHOULD BE NO OBSTACLES SO THIS IS A PROBLEM')
                    if self.world.collides(a, b):
                        distance = self.world.get_distance(a, b)
                        a.agent_collision_rew[
                            distance <= self.min_collision_distance
                        ] += self.agent_collision_penalty

        pos_reward = (
            self.pos_rew if self.shared_rew else agent.pos_rew
        )  # Choose global or local reward based on configuration
        return pos_reward + self.final_rew + agent.agent_collision_rew
    
    def agent_reward(self, agent: Agent):
        agent.distance_to_goal = torch.linalg.vector_norm(
            agent.state.pos - agent.goal.state.pos,
            dim=-1,
        )
        # agent.on_goal = agent.distance_to_goal < agent.goal.shape.radius

        pos_shaping = agent.distance_to_goal #* self.pos_shaping_factor
        agent.pos_rew = agent.pos_shaping - pos_shaping
        agent.pos_shaping = pos_shaping
        return agent.pos_rew

    def observation(self, agent: Agent):
        goal_poses = []
        self.observe_all_goals = False
        self.collisions = False
        if self.observe_all_goals:
            for a in self.world.agents:
                goal_poses.append(agent.state.pos - a.goal.state.pos)
        else:
            goal_poses.append(agent.state.pos - agent.goal.state.pos)
        new_obs = torch.cat(
            [
                agent.state.pos,
                agent.state.vel,
            ]
            + goal_poses
            + (
                [agent.sensors[0]._max_range - agent.sensors[0].measure()]
                if self.collisions
                else []
            ),
            dim=-1,
        )
        obs = {
            "obs": new_obs,
            "pos": agent.state.pos,
            "vel": agent.state.vel,
        }
        if not isinstance(agent.dynamics, Holonomic):
            # Non hoonomic agents need to know angular states
            obs.update(
                {
                    "rot": agent.state.rot,
                    "ang_vel": agent.state.ang_vel,
                }
            )
        return obs
    # def observation(self, agent: Agent):
    #     goal_poses = []
    #     self.observe_all_goals = False
    #     self.collisions = False
    #     if self.observe_all_goals:
    #         for a in self.world.agents:
    #             goal_poses.append(agent.state.pos - a.goal.state.pos)
    #     else:
    #         goal_poses.append(agent.state.pos - agent.goal.state.pos)
    #     return torch.cat(
    #         [
    #             agent.state.pos,
    #             agent.state.vel,
    #         ]
    #         + goal_poses
    #         + (
    #             [agent.sensors[0]._max_range - agent.sensors[0].measure()]
    #             if self.collisions
    #             else []
    #         ),
    #         dim=-1,
    #     )

    def done(self) -> Tensor:
        return self.all_goal_reached

    def info(self, agent: Agent) -> Dict[str, Tensor]:
        return {
            "pos_rew": self.pos_rew if self.shared_rew else agent.pos_rew,
            "final_rew": self.final_rew,
            "agent_collision_rew": agent.agent_collision_rew,
        }

    def extra_render(self, env_index: int = 0) -> "List[Geom]":
        from vmas.simulator import rendering

        geoms = [
            ScenarioUtils.plot_entity_rotation(agent, env_index)
            for agent in self.world.agents
            if not isinstance(agent.dynamics, Holonomic)
        ]  # Plot the rotation for non-holonomic agents

        # Plot communication lines
        if self.comms_rendering_range > 0:
            for i, agent1 in enumerate(self.world.agents):
                for j, agent2 in enumerate(self.world.agents):
                    if j <= i:
                        continue
                    agent_dist = torch.linalg.vector_norm(
                        agent1.state.pos - agent2.state.pos, dim=-1
                    )
                    if agent_dist[env_index] <= self.comms_rendering_range:
                        color = Color.BLACK.value
                        line = rendering.Line(
                            (agent1.state.pos[env_index]),
                            (agent2.state.pos[env_index]),
                            width=1,
                        )
                        line.set_color(*color)
                        geoms.append(line)
        return geoms

"""## Adding our task to BenchMARL

Tasks in BenchMARL are `Enum`s.

This is so that you are able to use python autocompletion to choose from a discrete set of tasks and ease benchmarking.

Each environment/simulator compatible with BenchMARL is an enum with alements all its tasks.

This allows you to do stuff like the following:
"""

from benchmarl.environments import VmasTask, Smacv2Task, PettingZooTask, MeltingPotTask
# VmasTask.BALANCE # Try deleting the enum element name and see all the available ones
# Smacv2Task.PROTOSS_10_VS_10 # Try deleting the enum element name and see all the available ones
# PettingZooTask.MULTIWALKER # Try deleting the enum element name and see all the available ones
# MeltingPotTask.COMMONS_HARVEST__OPEN # Try deleting the enum element name and see all the available ones

"""Cool uh?

To do this, BenchMARL needs to be quite stringent. Thus, to add a new task, you need to either:

1.   Add an enum element if the task is from an existing simulator
2.   Add a new task enum if it is from a new simulator

This will require you to also add a yaml config for the task and a schema python dataclass to validate this config (we really want to be strict in this process not to allow any bugs).

The full process is described in high detail here: https://github.com/facebookresearch/BenchMARL/tree/main/examples/extending/task

Now, we are in a notebook, so we cannot edit the BenchMARL files and add enum elements easily, so we will cheat slightly and make BenchMARL think that our newly created task is one of the already avaialble ones in the liibrary.

To do this, we override the `get_env_fun` of `VmasTask` to say: "if the user requests the NAVIGATION task, don't use the dafault one from VMAS, but use the one we just created".

I hope you will accept this white lie for the puropose of adding a new task in a notebook :) .
"""

import copy
from typing import Callable, Optional
from benchmarl.environments import VmasTask
from benchmarl.utils import DEVICE_TYPING
from torchrl.envs import EnvBase, VmasEnv

def get_env_fun(
    self,
    num_envs: int,
    continuous_actions: bool,
    seed: Optional[int],
    device: DEVICE_TYPING,
) -> Callable[[], EnvBase]:
    config = copy.deepcopy(self.config)
    if (hasattr(self, "name") and self.name is "NAVIGATION") or (
        self is VmasTask.NAVIGATION
    ):  # This is the only modification we make ....
        scenario = MyScenario()  # .... ends here
    else:
        scenario = self.name.lower()
    return lambda: VmasEnv(
        scenario=scenario,
        num_envs=num_envs,
        continuous_actions=continuous_actions,
        seed=seed,
        device=device,
        categorical_actions=True,
        clamp_actions=True,
        **config,
    )

try:
    from benchmarl.environments import VmasClass
    VmasClass.get_env_fun = get_env_fun
except ImportError:
    VmasTask.get_env_fun = get_env_fun

"""## Training config

Let's first define some general parameters and then dive into creating each component
"""

# @title Devices
train_device = "cpu" if not torch.cuda.is_available() else "cuda:0" # @param {"type":"string"}
vmas_device = "cpu" if not torch.cuda.is_available() else "cuda:0" # @param {"type":"string"}

"""### Experiment config"""

from benchmarl.experiment import ExperimentConfig

# Loads from "benchmarl/conf/experiment/base_experiment.yaml"
experiment_config = ExperimentConfig.get_from_yaml() # We start by loading the defaults

# Override devices
experiment_config.sampling_device = vmas_device
experiment_config.train_device = train_device

experiment_config.max_n_frames = 18_000_000 # Number of frames before training ends
experiment_config.gamma = 0.99999
experiment_config.on_policy_collected_frames_per_batch = 120_000 # Number of frames collected each iteration
experiment_config.on_policy_n_envs_per_worker = 600 # Number of vmas vectorized enviornemnts (each will collect 100 steps, see max_steps in task_config -> 600 * 100 = 60_000 the number above)
experiment_config.on_policy_n_minibatch_iters = 45
experiment_config.on_policy_minibatch_size = 4096
experiment_config.evaluation = True
experiment_config.render = False
experiment_config.share_policy_params = True # Policy parameter sharing on
experiment_config.evaluation_interval = 120_000 # Interval in terms of frames, will evaluate every 120_000 / 60_000 = 2 iterations
experiment_config.evaluation_episodes = 200 # Number of vmas vectorized enviornemnts used in evaluation
experiment_config.loggers = ["csv"] # Log to csv, usually you should use wandb

# Checkpoint at every iteration
experiment_config.checkpoint_interval = (
    experiment_config.on_policy_collected_frames_per_batch
)

"""### Task config"""

# Loads from "benchmarl/conf/task/vmas/navigation.yaml"
task = VmasTask.NAVIGATION.get_from_yaml()

# We override the NAVIGATION config with ours
# task.config = {
#         "max_steps": 100,
#         "n_agents_holonomic": 2,
#         "n_agents_diff_drive": 1,
#         "n_agents_car": 1,
#         "lidar_range": 0.35,
#         "comms_rendering_range": 0,
#         "shared_rew": False,
# }

"""### Algorithm config"""

from benchmarl.algorithms import MappoConfig

# We can load from "benchmarl/conf/algorithm/mappo.yaml"
algorithm_config = MappoConfig.get_from_yaml()

# Or create it from scratch
algorithm_config = MappoConfig(
        share_param_critic=True, # Critic param sharing on
        clip_epsilon=0.2,
        entropy_coef=0.001, # We modify this, default is 0
        critic_coef=1,
        loss_critic_type="l2",
        lmbda=0.9,
        scale_mapping="biased_softplus_1.0", # Mapping for standard deviation
        use_tanh_normal=True,
        minibatch_advantage=False,
    )

"""### Model config"""

from benchmarl.models.mlp import MlpConfig

model_config = MlpConfig(
        num_cells=[32, 32], # Two layers with 32 neurons each
        layer_class=torch.nn.Linear,
        activation_class=torch.nn.Tanh,
    )

# Loads from "benchmarl/conf/model/layers/mlp.yaml" (in this case we use the defaults so it is the same)
model_config = MlpConfig.get_from_yaml()
critic_model_config = MlpConfig.get_from_yaml()

"""## Train with physically heterogeneous agents

We are now ready to train our scenario!

This is super simple, just a few lines of code.

We have already trained this for `50_000_000` frames with the wandb logger for you.

The intercative plots generated by BenchMARL are here: https://wandb.ai/matteobettini/nav_tutorial/runs/mappo_navigation_mlp__98e76926_25_02_05-11_05_26/workspace?nw=nwusermatteobettini
"""

from benchmarl.experiment import Experiment

# experiment_config.max_n_frames = 6_000 # Runs one iteration, change to 50_000_000 for full training
# experiment_config.on_policy_n_envs_per_worker = 60 # Remove this line for full training
# experiment_config.on_policy_n_minibatch_iters = 1 # Remove this line for full training

# experiment = Experiment(
#     task=task,
#     algorithm_config=algorithm_config,
#     model_config=model_config,
#     critic_model_config=critic_model_config,
#     seed=0,
#     config=experiment_config,
# )
# experiment.run()

"""Here are the agents after full training:

![](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/tutorial_gifs/Trainwithphysicallyheterogeneousagents.gif)

We will not go too much into the inner workings of BenchMARL.

One thing worth noting is that BenchMARL will automatically group agents based on the VMAS name (all cars togherther, all holonomic together, all differential drive together). This allows to stack their data because they are homogeneous.

Agents from different groups will be kept separate and it is even possible to use different models and algorithms for different groups. See https://github.com/facebookresearch/BenchMARL/tree/main/examples/ensemble

For more details on the grouping mechanism see https://www.youtube.com/watch?v=1tOIMgJf_VQ&t=1786s

## Train with physically homogenous agents

Now, let's change the task a bit, and let's have only one type of agent: the holonomic one.
"""

# task.config = {
#         "max_steps": 100,
#         "n_agents_holonomic": 4, # Changed
#         "n_agents_diff_drive": 0,  # Changed
#         "n_agents_car": 0,  # Changed
#         "lidar_range": 0.35,
#         "comms_rendering_range": 0,
#         "shared_rew": False,
# }

"""This is because in a few sections we will use the GNN model and GNNs work only within a group of agent. So doing this puts all our agents in one "holonomic" group.

Let's train again.

The intercative plots generated by BenchMARL are here: https://wandb.ai/matteobettini/nav_tutorial/runs/mappo_navigation_mlp__9375e68c_25_02_05-11_06_15/workspace?nw=nwusermatteobettini
"""

from benchmarl.experiment import Experiment

# experiment_config.max_n_frames = 6_000 # Runs one iteration, change to 50_000_000 for full training
# experiment_config.on_policy_n_envs_per_worker = 60 # Remove this line for full training
# experiment_config.on_policy_n_minibatch_iters = 1 # Remove this line for full training

# experiment = Experiment(
#     task=task,
#     algorithm_config=algorithm_config,
#     model_config=model_config,
#     critic_model_config=critic_model_config,
#     seed=0,
#     config=experiment_config,
# )
# experiment.run()

"""Here are the agents after full training:

![](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/tutorial_gifs/Trainwithphysicallyhomogenousagents.gif)

In the wandb panel you should see that the agents are doing the task pretty well!

Keep an eye on collision penalties by looking at the `collection/holonomic/info/agent_collision_rew panel`.

**Agents are learning to avoid collsions!**

## Train without LIDAR

Now, what happens if we disable the LIDAR? (set its range to 0)
"""

# task.config = {
#         "max_steps": 100,
#         "n_agents_holonomic": 4,
#         "n_agents_diff_drive": 0,
#         "n_agents_car": 0,
#         "lidar_range": 0, # Changed
#         "comms_rendering_range": 0,
#         "shared_rew": False,
# }

"""The result should be that the agents have now no way of avoiding each other.


The intercative plots generated by BenchMARL are here: https://wandb.ai/matteobettini/nav_tutorial/runs/mappo_navigation_mlp__84e35613_25_02_05-14_12_51/workspace?nw=nwusermatteobettini
"""

from benchmarl.experiment import Experiment

# experiment_config.max_n_frames = 6_000 # Runs one iteration, change to 50_000_000 for full training
# experiment_config.on_policy_n_envs_per_worker = 60 # Remove this line for full training
# experiment_config.on_policy_n_minibatch_iters = 1 # Remove this line for full training

# experiment = Experiment(
#     task=task,
#     algorithm_config=algorithm_config,
#     model_config=model_config,
#     critic_model_config=critic_model_config,
#     seed=0,
#     config=experiment_config,
# )
# experiment.run()

"""Here are the agents after full training:

![](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/tutorial_gifs/TrainwithoutLIDAR.gif)

Keep an eye on collision penalties by looking at the `collection/holonomic/info/agent_collision_rew` panel.

**Agents are not able to avoid collisions!**

## Train without LIDAR with GNN

So how do we solve this problem?

Our agents need to navigate to their goals and are not able to use their LIDARs anymore to avoid collisions.

Well, we could do something about this by allowing them to talk to each othter!

This, in fact, would be even better as instead of considering other agents' like dynamic obstacles trhough the LIDAR, with communication they should be able to share any information within the policy.

#### Graph Neural Networks (GNNs) for communication in MARL

Our solution is using GNNs as a layer in the agents' policy to allow information exchange.

The topology of the GNN can be built dynamically based on agent proximity (like traditional communication models). This is cool for 2 reasons:

1.   Agents can avoid talking to far away agents and thus avoid conditioning their input on them
2.   Agents just need to share information in neighbourhoods and thus you can avoid centralised training and execution is still decentralised.

We have used GNNs as policies in a number of papers: see https://arxiv.org/abs/2111.01777 and https://arxiv.org/abs/2301.07137

#### GNNs in BenchMARL

Let's build our GNN model in BenchMARL.

We first define the communicatiuon radius used to build the GNN topology at every step
"""

# @title Communication radius
comms_radius = 0 # @param {"type":"number"}

"""Let's define the GNN config.

We will tell the GNN where to read agent positions (`pos` in observation) and veleocities (`vel` in observation). We also tell the GNN that each of these keys has 2 features (VMAS is 2D).

Then, the GNN, will use them to build edge features, in paricular:
- Relative position $\mathbf{x}_i-\mathbf{x}_j$
- Distance $||\mathbf{x}_i-\mathbf{x}_j||$
- Relative velocity  $\mathbf{v}_i-\mathbf{v}_j$

These edge features are key for the agents to avoid collisions.

We can choose any GNN layer from [`torch_geometric.nn.conv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers) but we should choose one that uses edge features.

`GATv2Conv` is a pretty cool one that uses attention (like a transformer) to weight messages incoming from neigbours.

For more details on how the GNN works see [here](https://benchmarl.readthedocs.io/en/latest/generated/benchmarl.models.Gnn.html#benchmarl.models.Gnn) and the FAQs [here](https://github.com/facebookresearch/BenchMARL/issues/157)

"""

from benchmarl.models import GnnConfig, SequenceModelConfig
import torch_geometric

gnn_config = GnnConfig(
    topology="full", # Tell the GNN to build topology from positions and edge_radius
    self_loops=False,
    gnn_class=torch_geometric.nn.conv.GATv2Conv,
    gnn_kwargs={"add_self_loops": False, "residual": True}, # kwargs of GATv2Conv, residual is helpful in RL
    exclude_pos_from_node_features=False, # Do we want to use pos just to build edge features or also keep it in node features? Here we remove it as we want to be invariant to system translations (we do not use absolute positions)
)
# We add an MLP layer to process GNN output node embeddings into actions
mlp_config = MlpConfig.get_from_yaml()

# Chain them in a sequence
model_config = SequenceModelConfig(model_configs=[gnn_config, mlp_config], intermediate_sizes=[32])

"""And let's tell the VMAS task what comms_radius we are using so it can plot some nice communication lines when agents are in proximity.

This is just nice to have for visualization.

"""

task.config = {
        "max_steps": 250,
        "n_agents_holonomic": 4,
        "n_agents_diff_drive": 0,
        "n_agents_car": 0,
        "lidar_range": 0,
        "comms_rendering_range": comms_radius, # Changed
        "shared_rew": True,
}

"""We are now ready to train!

The intercative plots generated by BenchMARL are here: https://wandb.ai/matteobettini/nav_tutorial/runs/mappo_navigation_sequencemodel__ddd9fc72_25_02_05-11_14_42/workspace?nw=nwusermatteobettini
"""

from benchmarl.experiment import Experiment

# experiment_config.max_n_frames = 50_000_000 # Runs one iteration, change to 50_000_000 for full training
# experiment_config.on_policy_n_envs_per_worker = 60 # Remove this line for full training
# experiment_config.on_policy_n_minibatch_iters = 1 # Remove this line for full training

# experiment_config.evaluation_episodes = 1 # Number of vmas vectorized enviornemnts used in evaluation

episode_reward_mean_list = []
no_coll_rate = []
attn_loss = []
loss = []
success_rate = []
avg_rollout = []
outer_step_epoch = 0

from benchmarl.experiment.callback import Callback
from tensordict import TensorDict, TensorDictBase
class MyCallbackB(Callback):
    # def on_setup(self):
    #     print("Callback B is being called during setup")

    def on_evaluation_end(self, rollouts: List[TensorDictBase]):
        rollout_count = []
        success_count = 0
        no_coll_count = 0
        global avg_rollout
        global success_rate
        global no_coll_rate
        for x in rollouts:
            rollout_count.append(x.batch_size[0])
            if x.batch_size[0] < self.experiment.max_steps:
                success_count += 1
                if (x['holonomic']['info']['agent_collision_rew'].sum() == 0).item():
                    no_coll_count += 1
        avg_rollout.append(sum(rollout_count)/len(rollout_count))
        success_rate.append(float(success_count/len(rollouts)))
        no_coll_rate.append(float(no_coll_count/len(rollouts)))
        print('Average Makespan: ', avg_rollout[-1])
        print('Success Rate: ', success_rate[-1])
        print('No Collision Rate: ', no_coll_rate[-1])
        # print(f"Callback A is doing something with the evaluation rollouts {self.experiment.policy.module[0].module[0].module[0].models.module[0].gnns}")

# experiment = Experiment(
#     task=task,
#     algorithm_config=algorithm_config,
#     model_config=model_config,
#     critic_model_config=critic_model_config,
#     seed=0,
#     config=experiment_config,
#     callbacks=[MyCallbackB()]
# )

train_or_eval = input('Select TRAIN or EVAL: ')
if train_or_eval == 'TRAIN':
    experiment = Experiment(
        task=task,
        algorithm_config=algorithm_config,
        model_config=model_config,
        critic_model_config=critic_model_config,
        seed=0,
        config=experiment_config,
        callbacks=[MyCallbackB()]
    )
    experiment.run()
elif train_or_eval == 'EVAL':
    from pathlib import Path
    from benchmarl.hydra_config import reload_experiment_from_file
    # current_folder = Path(__file__).parent.parent.absolute()
    # print(current_folder)
    restore_file = input('Provide absolute path to folder to load: ')
    # restore_file = (
    #     current_folder
    #     / load_folder
    # )
    print(str(restore_file))
    # experiment = reload_experiment_from_file(str(restore_file))
    experiment_config.restore_file = str(restore_file)
    experiment = Experiment(
        task=task,
        algorithm_config=algorithm_config,
        model_config=model_config,
        critic_model_config=critic_model_config,
        seed=0,
        config=experiment_config,
        callbacks=[MyCallbackB()]
    )

    for param in experiment.policy.module[0].module[0].module[0].models.module[0].gnns[0].parameters():
        param.requires_grad = False
    
    policy_to_explain = copy.deepcopy(experiment.policy.module[0].module[0].module[0].models.module[0].gnns[0])


    from torch_geometric.explain import Explainer, GNNExplainer, DummyExplainer, GraphMaskExplainer, AttentionExplainer

    explainer_method = input('Select the explainer method (GraphMask, GNNExplainer, AttentionExplainer): ')

    if explainer_method == 'GraphMask':
        explainer_features = Explainer(
            model=policy_to_explain,
            algorithm=GraphMaskExplainer(1),
            explanation_type='model',
            node_mask_type='object',
            edge_mask_type='object',
            model_config=dict(
                mode='regression',
                task_level='node'
            ),
        )
    elif explainer_method == 'GNNExplainer':
        explainer_features = Explainer(
            model=policy_to_explain,
            algorithm=GNNExplainer(),
            explanation_type='model',
            node_mask_type='object',
            edge_mask_type='object',
            model_config=dict(
                mode='regression',
                task_level='node'
            ),
        )
    elif explainer_method == 'AttentionExplainer':
        explainer_features = Explainer(
            model=policy_to_explain,
            algorithm=AttentionExplainer(reduce='mean'),
            explanation_type='model',
            node_mask_type=None,
            edge_mask_type='object',
            model_config=dict(
                mode='regression',
                task_level='node'
            ),
        )
    else:
        raise Exception('Not a valid input')

    experiment.config.evaluation_episodes = 1
    experiment._setup_task()

    experiment.test_env.explainer_features = explainer_features
    experiment.test_env.gnn_exp = True

    experiment.evaluate()
else:
    raise Exception('Not valid input')


"""Here are the agents after full training:

![](https://raw.githubusercontent.com/matteobettini/vmas-media/refs/heads/main/media/tutorial_gifs/TrainwithoutLIDARwithGNN.gif)

As you can see the agents were able to use the GNN communication to avoid collisions

## Extensions

Here are a few things you can try in your own time.

**Beginner**:
- Change the algorithm and compare algorithms (algorithms that support only discrete actions will use those)
- Change the model. For example you can add a memory layer
- Train the agents with a shared reward by changing the task configuration

**Intermediate**:
- Try to disable parameter sharing think about its implications
- MAPPO uses a centralised critic which takes as input the concatenation of all agents observations. This is not scalable in the number of agents. Try using a [DeepSets](https://benchmarl.readthedocs.io/en/latest/generated/benchmarl.models.Deepsets.html#benchmarl.models.Deepsets) model instead for the critic
- Use different models or algorthms for each group using the [ensemble components](https://benchmarl.readthedocs.io/en/latest/concepts/features.html#ensemble-models-and-algorithms).

**Advanced**:
- Set up a reward curriculum using [callbacks](https://github.com/facebookresearch/BenchMARL/tree/main/examples/callback). Start from a dense reward and slowly anneal it leaving only the sparse one
- Implement [GPPO](https://arxiv.org/abs/2301.07137) by using a GNN for both the actor and critic and sharing the GNN layer parameters between actor and critic. Hint: you can use a callback and [this function](https://github.com/facebookresearch/BenchMARL/blob/7ae02107df151a4a2eb002d61ce3426c9c002cec/benchmarl/models/common.py#L165). You will also have to change the algorithm to IPPO
"""